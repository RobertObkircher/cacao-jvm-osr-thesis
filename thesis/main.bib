@inproceedings{10.1145/3486606.3486782,
  author = {D'Souza, Matt and Duboscq, Gilles},
  title = {Lightweight On-Stack Replacement in Languages with Unstructured Loops},
  year = {2021},
  isbn = {9781450391092},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3486606.3486782},
  doi = {10.1145/3486606.3486782},
  abstract = {On-stack replacement (OSR) is a popular technique used by just in time (JIT) compilers. A JIT can use OSR to transfer from interpreted to compiled code in the middle of execution, immediately reaping the performance benefits of compilation. This technique typically relies on loop counters, so it cannot be easily applied to languages with unstructured control flow. It is possible to reconstruct the high-level loop structures of an unstructured language using a control flow analysis, but such an analysis can be complicated, expensive, and language-specific. In this paper, we present a more lightweight strategy for OSR in unstructured languages which relies only on detecting backward jumps. We design a simple, language-agnostic API around this strategy for language interpreters. We then discuss our implementation of the API in the Truffle framework, and the design choices we made to make it efficient and correct. In our evaluation, we integrate the API with Truffle's LLVM bitcode interpreter, and find the technique is effective at improving start-up performance without harming warmed-up performance.},
  booktitle = {Proceedings of the 13th ACM SIGPLAN International Workshop on Virtual Machines and Intermediate Languages},
  pages = {4–13},
  numpages = {10},
  keywords = {unstructured loops, bytecode interpreter, Truffle, on-stack replacement, partial evaluation},
  location = {Chicago, IL, USA},
  series = {VMIL 2021}
}

@inproceedings{10.1145/3192366.3192396,
  author = {D'Elia, Daniele Cono and Demetrescu, Camil},
  title = {On-Stack Replacement, Distilled},
  year = {2018},
  isbn = {9781450356985},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3192366.3192396},
  doi = {10.1145/3192366.3192396},
  abstract = {On-stack replacement (OSR) is essential technology for adaptive optimization, allowing changes to code actively executing in a managed runtime. The engineering aspects of OSR are well-known among VM architects, with several implementations available to date. However, OSR is yet to be explored as a general means to transfer execution between related program versions, which can pave the road to unprecedented applications that stretch beyond VMs. We aim at filling this gap with a constructive and provably correct OSR framework, allowing a class of general-purpose transformation functions to yield a special-purpose replacement. We describe and evaluate an implementation of our technique in LLVM. As a novel application of OSR, we present a feasibility study on debugging of optimized code, showing how our techniques can be used to fix variables holding incorrect values at breakpoints due to optimizations.},
  booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages = {166–180},
  numpages = {15},
  keywords = {debugging, dynamic compilers, deoptimization},
  location = {Philadelphia, PA, USA},
  series = {PLDI 2018}
}

@misc{JavaSE,
  key = {JSE},
  title = {{Java SE} Specifications},
  note  = {\url{https://docs.oracle.com/javase/specs/index.html}, last accessed on \mbox{2024-10-05}},
}

@article{MethodInlining,
  author = {Tobias Schwarzinger},
  title = {Method Inlining in the Second Stage Compiler of the {CACAO VM}},
  year = {2020},
}

@misc{CLR-OSR,
  key = {CLR},
  title = {On Stack Replacement in the {CLR}},
  note  = {\url{https://github.com/dotnet/runtime/blob/9866d1285dcf2448c966edbf02b8c17585d430fb/docs/design/features/OnStackReplacement.md}, last accessed on \mbox{2024-10-05}},
}

@misc{cacao-repo,
    key = {CRE},
    title = {{CACAO VM} source code},
    note  = {\url{https://bitbucket.org/cacaovm/cacao/}, last accessed on \mbox{2024-10-05}},
}

@misc{cacao-homepage,
  key = {CHP},
  title = {{CACAO VM} homepage},
  note  = {\url{http://www.cacaojvm.org/}, last accessed on \mbox{2024-10-05}},
}

@misc{specjvm2008,
  key = {SPEC},
  title = {{SPECjvm} 2008},
  note = {\url{https://www.spec.org/jvm2008/}, last accessed on \mbox{2024-10-05}}
}

@article{AdapOptSurvey,
  author = {Arnold, Matthew and Fink, Stephen and Grove, David and Hind, Michael and Sweeney, Peter},
  year = {2005},
  month = {02},
  pages = {449-466},
  title = {A Survey of Adaptive Optimization in Virtual Machines},
  volume = {93},
  journal = {Proceedings of the IEEE},
  doi = {10.1109/JPROC.2004.840305}
}

@inproceedings{10.1145/1294325.1294356,
  author = {Steiner, Edwin and Krall, Andreas and Thalinger, Christian},
  title = {Adaptive Inlining and On-Stack Replacement in the {CACAO} Virtual Machine},
  year = {2007},
  isbn = {9781595936721},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1294325.1294356},
  doi = {10.1145/1294325.1294356},
  abstract = {Method inlining is a well-known and effective optimization technique for object-oriented programs. In the context of dynamic compilation, method inlining can be used as an adaptive optimization in order to eliminate the overhead of frequently executed calls. This work presents an implementation of method inlining in the CACAO virtual machine. On-stack replacement is used for installing optimized code and for deoptimizing code when optimistic assumptions of the optimizer are broken by dynamic class loading. Three inlining heuristics are compared using empirical results from a set of benchmark programs. The best heuristic eliminates 51.5% up to 99.96% of all executed calls and improves execution time up to 18%.},
  booktitle = {Proceedings of the 5th International Symposium on Principles and Practice of Programming in Java},
  pages = {221–226},
  numpages = {6},
  keywords = {just-in-time compiler, method inlining, on-stack replacement, virtual machines},
  location = {Lisboa, Portugal},
  series = {PPPJ '07}
}

@book{EislJosef2013Offt,
  year = {2013},
  title = {Optimization framework for the {CACAO VM}},
  language = {eng},
  author = {Eisl, Josef},
  doi = {10.34726/hss.2013.23350},
}

@article{cacao-64bit-jit,
  author = {Krall, Andreas and Grafl, Reinhard},
  title = {{CACAO} — A 64-bit {JavaVM} just-in-time compiler},
  journal = {Concurrency: Practice and Experience},
  volume = {9},
  number = {11},
  pages = {1017-1030},
  doi = {10.1002/(SICI)1096-9128(199711)9:11<1017::AID-CPE347>3.0.CO;2-0},
  abstract = {Abstract This paper describes the design and implementation of CACAO, a just-in-time compiler for Java. The CACAO system translates Java byte code on demand into native code for the ALPHA processor. During this translation process the stack-oriented Java byte code is transformed into a register-oriented intermediate code. Local variables and stack locations are replaced by pseudo-registers eliminating the 32-bit restriction on address types. A fast register allocation algorithm is applied to map the pseudo-registers to machine registers. During code generation, field offsets are computed for proper alignment on 64-bit architectures. Even though the CACAO system has to incur loading and compilation time, it executes Java programs up to 85 times faster than the JDK interpreter, and up to seven times faster than the kaffe JIT compiler. It is slightly slower than equivalent C programs compiled at the highest optimization level. © 1997 John Wiley \& Sons, Ltd.},
  year = {1997}
}

@article{10.1145/191081.191116,
  author = {H\"{o}lzle, Urs and Ungar, David},
  title = {A Third-Generation {SELF} Implementation: Reconciling Responsiveness with Performance},
  year = {1994},
  issue_date = {Oct. 1994},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {29},
  number = {10},
  issn = {0362-1340},
  url = {https://doi.org/10.1145/191081.191116},
  doi = {10.1145/191081.191116},
  abstract = {Programming systems should be both responsive (to support rapid development) and efficient (to complete computations quickly). Pure object-oriented languages are harder to implement efficiently since they need optimization to achieve good performance. Unfortunately, optimization conflicts with interactive responsiveness because it tends to produce long compilation pauses, leading to unresponsive programming environments. Therefore, to achieve good responsiveness, existing exploratory programming environments such as the Smalltalk-80 environment rely on interpretation or non-optimizing dynamic compilation. But such systems pay a price for their interactiveness, since they may execute programs several times slower than an optimizing system.SELF-93 reconciles high performance with responsiveness by combining a fast, non-optimizing compiler with a slower, optimizing compiler. The resulting system achieves both excellent performance (two or three times faster than existing Smalltalk systems) and good responsiveness. Except for situations requiring large applications to be (re)compiled from scratch, the system allows for pleasant interactive use with few perceptible compilation pauses. To our knowledge, SELF-93 is the first implementation of a pure object-oriented language achieving both good performance and good responsiveness.When measuring interactive pauses, it is imperative to treat multiple short pauses as one longer pause if the pauses occur in short succession, since they are perceived as one pause by the user. We propose a definition of pause clustering and show that clustering can make an order-of-magnitude difference in the pause time distribution.},
  journal = {SIGPLAN Not.},
  month = {oct},
  pages = {229–243},
  numpages = {15}
}


@article{dominators,
  author = {Lengauer, Thomas and Tarjan, Robert Endre},
  title = {A fast algorithm for finding dominators in a flowgraph},
  year = {1979},
  issue_date = {July 1979},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {1},
  number = {1},
  issn = {0164-0925},
  url = {https://doi.org/10.1145/357062.357071},
  doi = {10.1145/357062.357071},
  abstract = {A fast algorithm for finding dominators in a flowgraph is presented. The algorithm uses
  depth-first search and an efficient method of computing functions defined on paths in trees. A simple implementation of the algorithm runs in O(m log n) time, where m is the number of edges and n is the number of vertices in the problem graph. A more sophisticated implementation runs in O(mα(m, n)) time, where α(m, n) is a functional inverse of Ackermann's function.Both versions of the algorithm were implemented in Algol W, a Stanford University version of Algol, and tested on an IBM 370/168. The programs were compared with an implementation by Purdom and Moore of a straightforward O(mn)-time algorithm, and with a bit vector algorithm described by Aho and Ullman. The fast algorithm beat the straightforward algorithm and the bit vector algorithm on all but the smallest graphs tested.},
  journal = {ACM Trans. Program. Lang. Syst.},
  month = jan,
  pages = {121–141},
  numpages = {21}
}

@inproceedings{hotspot,
  author = {Paleczny, Michael and Vick, Christopher and Click, Cliff},
  year = {2001},
  month = {01},
  pages = {},
  title = {The Java HotSpot Server Compiler.}
}

@mastersthesis{osr-jllvm,
author  = "Markus Böck",
title   = "Implementing On-Stack-Replacement in an LLVM-based JVM",
school  = "TU Wien",
year    = "2024",
type={Bachelor's Thesis}
}

@misc{jllvm,
  key = {JLLVM},
  title = {JLLVM},
  note  = {\url{https://github.com/JLLVM/JLLVM}, last accessed on \mbox{2024-10-30}},
}

@inproceedings{truffle,
  author = {Wimmer, Christian and W\"{u}rthinger, Thomas},
  title = {Truffle: a self-optimizing runtime system},
  year = {2012},
  isbn = {9781450315630},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2384716.2384723},
  doi = {10.1145/2384716.2384723},
  abstract = {We present Truffle, a novel framework for implementing managed languages in Java™. The language implementer writes an AST interpreter, which is integrated in our framework that allows tree rewriting during AST interpretation. Tree rewrites incorporate type feedback and other profiling information into the tree, thus specializing the tree and augmenting it with run-time information. When the tree reaches a stable state, partial evaluation compiles the tree into optimized machine code. The partial evaluation is done by Graal, the just-in-time compiler of our Java VM (a variation of the Java HotSpot VM). To show that Truffle supports a variety of programming language paradigms, we present prototype implementations of JavaScript (a dynamically typed programming language) and J (an array programming language).},
  booktitle = {Proceedings of the 3rd Annual Conference on Systems, Programming, and Applications: Software for Humanity},
  pages = {13–14},
  numpages = {2},
  keywords = {dynamic languages, graal, j, java, javascript, language implementation, virtual machine, truffle},
  location = {Tucson, Arizona, USA},
  series = {SPLASH '12}
}

@inproceedings{graalvm,
  author = {W\"{u}rthinger, Thomas and Wimmer, Christian and W\"{o}\ss{}, Andreas and Stadler, Lukas and Duboscq, Gilles and Humer, Christian and Richards, Gregor and Simon, Doug and Wolczko, Mario},
  title = {One VM to rule them all},
  year = {2013},
  isbn = {9781450324724},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2509578.2509581},
  doi = {10.1145/2509578.2509581},
  abstract = {Building high-performance virtual machines is a complex and expensive undertaking; many popular languages still have low-performance implementations. We describe a new approach to virtual machine (VM) construction that amortizes much of the effort in initial construction by allowing new languages to be implemented with modest additional effort. The approach relies on abstract syntax tree (AST) interpretation where a node can rewrite itself to a more specialized or more general node, together with an optimizing compiler that exploits the structure of the interpreter. The compiler uses speculative assumptions and deoptimization in order to produce efficient machine code. Our initial experience suggests that high performance is attainable while preserving a modular and layered architecture, and that new high-performance language implementations can be obtained by writing little more than a stylized interpreter.},
  booktitle = {Proceedings of the 2013 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming \& Software},
  pages = {187–204},
  numpages = {18},
  keywords = {dynamic languages, java, javascript, language implementation, optimization, virtual machine},
  location = {Indianapolis, Indiana, USA},
  series = {Onward! 2013}
}

@inproceedings{deopt-grouping,
  author = {Duboscq, Gilles and W\"{u}rthinger, Thomas and M\"{o}ssenb\"{o}ck, Hanspeter},
  title = {Speculation without regret: reducing deoptimization meta-data in the Graal compiler},
  year = {2014},
  isbn = {9781450329262},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2647508.2647521},
  doi = {10.1145/2647508.2647521},
  abstract = {Speculative optimizations are used in most Just In Time (JIT) compilers in order to take advantage of dynamic runtime feedback. These speculative optimizations usually require the compiler to produce meta-data that the Virtual Machine (VM) can use as fallback when a speculation fails. This meta-data can be large and incurs a significant memory overhead since it needs to be stored alongside the machine code for as long as the machine code lives. The design of the Graal compiler leads to many speculations falling back to a similar state and location. In this paper we present deoptimization grouping, an optimization using this property of the Graal compiler to reduce the amount of meta-data that must be stored by the VM without having to modify the VM. We compare our technique with existing meta-data compression techniques from the HotSpot Virtual Machine and study how well they combine. In order to make informed decisions about speculation meta-data, we present an empirical analysis of the origin, impact and usages of this meta-data.},
  booktitle = {Proceedings of the 2014 International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools},
  pages = {187–193},
  numpages = {7},
  keywords = {Java virtual machine, just-in-time compilation, metadata, speculative optimization},
  location = {Cracow, Poland},
  series = {PPPJ '14}
}

@inproceedings{10.1145/2451512.2451541,
  author = {Lameed, Nurudeen A. and Hendren, Laurie J.},
  title = {A modular approach to on-stack replacement in LLVM},
  year = {2013},
  isbn = {9781450312660},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2451512.2451541},
  doi = {10.1145/2451512.2451541},
  abstract = {On-stack replacement (OSR) is a technique that allows a virtual machine to interrupt running code during the execution of a function/method, to re-optimize the function on-the-fly using an optimizing JIT compiler, and then to resume the interrupted function at the point and state at which it was interrupted. OSR is particularly useful for programs with potentially long-running loops, as it allows dynamic optimization of those loops as soon as they become hot.This paper presents a modular approach to implementing OSR for the LLVM compiler infrastructure. This is an important step forward because LLVM is gaining popular support, and adding the OSR capability allows compiler developers to develop new dynamic techniques. In particular, it will enable more sophisticated LLVM-based JIT compiler approaches. Indeed, other compiler/VM developers can use our approach because it is a clean modular addition to the standard LLVM distribution. Further, our approach is defined completely at the LLVM-IR level and thus does not require any modifications to the target code generation.The OSR implementation can be used by different compilers to support a variety of dynamic optimizations. As a demonstration of our OSR approach, we have used it to support dynamic inlining in McVM. McVM is a virtual machine for MATLAB which uses a LLVM-based JIT compiler. MATLAB is a popular dynamic language for scientific and engineering applications that typically manipulate large matrices and often contain long-running loops, and is thus an ideal target for dynamic JIT compilation and OSRs. Using our McVM example, we demonstrate reasonable overheads for our benchmark set, and performance improvements when using it to perform dynamic inlining.},
  booktitle = {Proceedings of the 9th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
  pages = {143–154},
  numpages = {12},
  keywords = {on-stack replacement, mcjit, matlab, llvm, jit compilation, dynamic optimization},
  location = {Houston, Texas, USA},
  series = {VEE '13}
}

@article{llvm-osr-1,
  author = {Lameed, Nurudeen A. and Hendren, Laurie J.},
  title = {A modular approach to on-stack replacement in LLVM},
  year = {2013},
  issue_date = {July 2013},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {48},
  number = {7},
  issn = {0362-1340},
  url = {https://doi.org/10.1145/2517326.2451541},
  doi = {10.1145/2517326.2451541},
  abstract = {On-stack replacement (OSR) is a technique that allows a virtual machine to interrupt running code during the execution of a function/method, to re-optimize the function on-the-fly using an optimizing JIT compiler, and then to resume the interrupted function at the point and state at which it was interrupted. OSR is particularly useful for programs with potentially long-running loops, as it allows dynamic optimization of those loops as soon as they become hot.This paper presents a modular approach to implementing OSR for the LLVM compiler infrastructure. This is an important step forward because LLVM is gaining popular support, and adding the OSR capability allows compiler developers to develop new dynamic techniques. In particular, it will enable more sophisticated LLVM-based JIT compiler approaches. Indeed, other compiler/VM developers can use our approach because it is a clean modular addition to the standard LLVM distribution. Further, our approach is defined completely at the LLVM-IR level and thus does not require any modifications to the target code generation.The OSR implementation can be used by different compilers to support a variety of dynamic optimizations. As a demonstration of our OSR approach, we have used it to support dynamic inlining in McVM. McVM is a virtual machine for MATLAB which uses a LLVM-based JIT compiler. MATLAB is a popular dynamic language for scientific and engineering applications that typically manipulate large matrices and often contain long-running loops, and is thus an ideal target for dynamic JIT compilation and OSRs. Using our McVM example, we demonstrate reasonable overheads for our benchmark set, and performance improvements when using it to perform dynamic inlining.},
  journal = {SIGPLAN Not.},
  month = mar,
  pages = {143–154},
  numpages = {12},
  keywords = {on-stack replacement, mcjit, matlab, llvm, jit compilation, dynamic optimization}
}

@inproceedings{llvm-osr-2,
  author = {D'Elia, Daniele Cono and Demetrescu, Camil},
  title = {Flexible on-stack replacement in LLVM},
  year = {2016},
  isbn = {9781450337786},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2854038.2854061},
  doi = {10.1145/2854038.2854061},
  abstract = {On-Stack Replacement (OSR) is a technique for dynamically transferring execution between different versions of a function at run time. OSR is typically used in virtual machines to interrupt a long-running function and recompile it at a higher optimization level, or to replace it with a different one when a speculative assumption made during its compilation no longer holds. In this paper we present a framework for OSR that introduces novel ideas and combines features of existing techniques that no previous solution provided simultaneously. New features include OSR with compensation code to adjust the program state during a transition and the ability to fire an OSR from arbitrary locations in the code. Our approach is platform-independent as the OSR machinery is entirely encoded at a compiler’s intermediate representation level. We implement and evaluate our technique in the LLVM compiler infrastructure, which is gaining popularity as Just-In-Time (JIT) compiler in virtual machines for dynamic languages such as Javascript, MATLAB, Python, and Ruby. As a case study of our approach, we show how to improve the state of the art in the optimization of the feval instruction, a performance-critical construct of the MATLAB language.},
  booktitle = {Proceedings of the 2016 International Symposium on Code Generation and Optimization},
  pages = {250–260},
  numpages = {11},
  keywords = {just-in-time compilation, deoptimization, code optimization, On-stack replacement, LLVM},
  location = {Barcelona, Spain},
  series = {CGO '16}
}
